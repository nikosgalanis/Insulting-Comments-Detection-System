{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insulting Comments Detection System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project is to create a machine learning system that takes as input a comment, and ranks it as insulting or neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package genesis to /home/nikos/nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/nikos/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/nikos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/nikos/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Ignoring unnecessory warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  \n",
    "# Specialized container datatypes\n",
    "import collections\n",
    "# For Map vizualization\n",
    "import folium\n",
    "from nltk.corpus import genesis\n",
    "# For data vizualization \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "# For large and multi-dimensional arrays\n",
    "import numpy as np\n",
    "# For data manipulation and analysis\n",
    "import pandas as pd\n",
    "# Natural language processing library\n",
    "import nltk\n",
    "nltk.download('genesis')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import (\n",
    "    BigramAssocMeasures,\n",
    "    BigramCollocationFinder)\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "# For image processing\n",
    "from PIL import Image, ImageOps\n",
    "# For random selection \n",
    "import random\n",
    "# For basic cleaning and data preprocessing \n",
    "import re\n",
    "import string \n",
    "# Communicating with operating and file system\n",
    "import os\n",
    "# Machine learning libary\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support \n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, normalize, label_binarize\n",
    "from sklearn.svm import SVC\n",
    "# For wordcloud generating \n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing files and creating datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a directory with all the data, we create 2 different data frames; one for training our algorithms, and one for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/impermium_verification_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3947 entries, 0 to 3946\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Insult   3947 non-null   int64 \n",
      " 1   Date     3229 non-null   object\n",
      " 2   Comment  3947 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 92.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618192155Z</td>\n",
       "      <td>\"You fuck your dad.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528192215Z</td>\n",
       "      <td>\"i really don't understand your point.\\xa0 It ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"A\\\\xc2\\\\xa0majority of Canadians can and has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"listen if you dont wanna get married to a man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20120619094753Z</td>\n",
       "      <td>\"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Insult             Date                                            Comment\n",
       "0       1  20120618192155Z                               \"You fuck your dad.\"\n",
       "1       0  20120528192215Z  \"i really don't understand your point.\\xa0 It ...\n",
       "2       0              NaN  \"A\\\\xc2\\\\xa0majority of Canadians can and has ...\n",
       "3       0              NaN  \"listen if you dont wanna get married to a man...\n",
       "4       0  20120619094753Z  \"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.info()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2235 entries, 0 to 2234\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   id       2235 non-null   int64 \n",
      " 1   Insult   2235 non-null   int64 \n",
      " 2   Date     2235 non-null   object\n",
      " 3   Comment  2235 non-null   object\n",
      " 4   Usage    2235 non-null   object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 87.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20120603163526Z</td>\n",
       "      <td>\"like this if you are a tribe fan\"</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20120531215447Z</td>\n",
       "      <td>\"you're idiot.......................\"</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>20120823164228Z</td>\n",
       "      <td>\"I am a woman Babs, and the only \"war on women...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>20120826010752Z</td>\n",
       "      <td>\"WOW &amp; YOU BENEFITTED SO MANY WINS THIS YEAR F...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>20120602223825Z</td>\n",
       "      <td>\"haha green me red you now loser whos winning ...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Insult             Date  \\\n",
       "0   1       0  20120603163526Z   \n",
       "1   2       1  20120531215447Z   \n",
       "2   3       1  20120823164228Z   \n",
       "3   4       1  20120826010752Z   \n",
       "4   5       1  20120602223825Z   \n",
       "\n",
       "                                             Comment        Usage  \n",
       "0                 \"like this if you are a tribe fan\"  PrivateTest  \n",
       "1              \"you're idiot.......................\"  PrivateTest  \n",
       "2  \"I am a woman Babs, and the only \"war on women...  PrivateTest  \n",
       "3  \"WOW & YOU BENEFITTED SO MANY WINS THIS YEAR F...  PrivateTest  \n",
       "4  \"haha green me red you now loser whos winning ...  PrivateTest  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.info()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and sanitizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we have to preprocess our training and test set. We are going to convert everything to lowercase, and remove any punctuation points, weird characters and links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_normalization(text):\n",
    "    # convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # remove all special characters, punctuation and spaces from string\n",
    "    text = re.sub('\\n|\\r|\\t', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]+', '', text)\n",
    "    # first group of special chars: \\u followed by a number\n",
    "    text = re.sub('u\\d\\w+', '', text)\n",
    "    # second group: \\x followed by a letter\n",
    "    text = re.sub('x[a-z]\\d', '', text)\n",
    "    # remove links\n",
    "    text = re.sub(r'^http?://', ' ', text)\n",
    "    text = re.sub(r'^www://', ' ', text)\n",
    "    # return normalized text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_df, is_test):\n",
    "    if (is_test == False):\n",
    "        # Remove rows with missing values in column col\n",
    "        input_df.dropna(inplace=True)\n",
    "    # Speed up code using numpy vectorization\n",
    "    vfunc = np.vectorize(text_normalization)\n",
    "    input_df.Comment = vfunc(input_df.Comment.values)\n",
    "    # return processed input_df\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the above preprocessing techniques on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618192155Z</td>\n",
       "      <td>you fuck your dad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528192215Z</td>\n",
       "      <td>i really dont understand your point it seems t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20120619094753Z</td>\n",
       "      <td>cc b xu  bi txecnh 2011 c n ho khng  ncc ng dn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>20120620171226Z</td>\n",
       "      <td>sdl ok but i would hope theyd sign him to a on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>20120503012628Z</td>\n",
       "      <td>yeah and where are you now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3942</th>\n",
       "      <td>1</td>\n",
       "      <td>20120502172717Z</td>\n",
       "      <td>you are both morons and that is never happening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3943</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528164814Z</td>\n",
       "      <td>many toolbars include spell check like yahoo f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3944</th>\n",
       "      <td>0</td>\n",
       "      <td>20120620142813Z</td>\n",
       "      <td>lambeauorwrigleykmossnsioux falls sd i told my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528205648Z</td>\n",
       "      <td>how about felix he is sure turning into one he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>0</td>\n",
       "      <td>20120515200734Z</td>\n",
       "      <td>youre all upset defending this hipster bandand...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3229 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Insult             Date  \\\n",
       "0          1  20120618192155Z   \n",
       "1          0  20120528192215Z   \n",
       "4          0  20120619094753Z   \n",
       "5          0  20120620171226Z   \n",
       "6          0  20120503012628Z   \n",
       "...      ...              ...   \n",
       "3942       1  20120502172717Z   \n",
       "3943       0  20120528164814Z   \n",
       "3944       0  20120620142813Z   \n",
       "3945       0  20120528205648Z   \n",
       "3946       0  20120515200734Z   \n",
       "\n",
       "                                                Comment  \n",
       "0                                     you fuck your dad  \n",
       "1     i really dont understand your point it seems t...  \n",
       "4     cc b xu  bi txecnh 2011 c n ho khng  ncc ng dn...  \n",
       "5     sdl ok but i would hope theyd sign him to a on...  \n",
       "6                            yeah and where are you now  \n",
       "...                                                 ...  \n",
       "3942    you are both morons and that is never happening  \n",
       "3943  many toolbars include spell check like yahoo f...  \n",
       "3944  lambeauorwrigleykmossnsioux falls sd i told my...  \n",
       "3945  how about felix he is sure turning into one he...  \n",
       "3946  youre all upset defending this hipster bandand...  \n",
       "\n",
       "[3229 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_train_df = preprocess(train_df, False)\n",
    "preprocessed_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply now the above preprocessing techniques on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20120603163526Z</td>\n",
       "      <td>like this if you are a tribe fan</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20120531215447Z</td>\n",
       "      <td>youre idiot</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>20120823164228Z</td>\n",
       "      <td>i am a woman babs and the only war on women i ...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>20120826010752Z</td>\n",
       "      <td>wow  you benefitted so many wins this year fro...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>20120602223825Z</td>\n",
       "      <td>haha green me red you now loser whos winning n...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2230</th>\n",
       "      <td>2231</td>\n",
       "      <td>0</td>\n",
       "      <td>20120528100303Z</td>\n",
       "      <td>fuckin lame come on wtf stop fucking over my b...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2231</th>\n",
       "      <td>2232</td>\n",
       "      <td>1</td>\n",
       "      <td>20120531185813Z</td>\n",
       "      <td>you shut your ignorant pie hole you little ins...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2232</th>\n",
       "      <td>2233</td>\n",
       "      <td>0</td>\n",
       "      <td>20120529130822Z</td>\n",
       "      <td>sweetie pie is looking very much like her cous...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2233</th>\n",
       "      <td>2234</td>\n",
       "      <td>1</td>\n",
       "      <td>20120531045826Z</td>\n",
       "      <td>ball4real where are you with your miami gayness</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2234</th>\n",
       "      <td>2235</td>\n",
       "      <td>0</td>\n",
       "      <td>20120531184524Z</td>\n",
       "      <td>manif you are a 3 point shooter you must love ...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2235 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  Insult             Date  \\\n",
       "0        1       0  20120603163526Z   \n",
       "1        2       1  20120531215447Z   \n",
       "2        3       1  20120823164228Z   \n",
       "3        4       1  20120826010752Z   \n",
       "4        5       1  20120602223825Z   \n",
       "...    ...     ...              ...   \n",
       "2230  2231       0  20120528100303Z   \n",
       "2231  2232       1  20120531185813Z   \n",
       "2232  2233       0  20120529130822Z   \n",
       "2233  2234       1  20120531045826Z   \n",
       "2234  2235       0  20120531184524Z   \n",
       "\n",
       "                                                Comment        Usage  \n",
       "0                      like this if you are a tribe fan  PrivateTest  \n",
       "1                                           youre idiot  PrivateTest  \n",
       "2     i am a woman babs and the only war on women i ...  PrivateTest  \n",
       "3     wow  you benefitted so many wins this year fro...  PrivateTest  \n",
       "4     haha green me red you now loser whos winning n...  PrivateTest  \n",
       "...                                                 ...          ...  \n",
       "2230  fuckin lame come on wtf stop fucking over my b...  PrivateTest  \n",
       "2231  you shut your ignorant pie hole you little ins...  PrivateTest  \n",
       "2232  sweetie pie is looking very much like her cous...  PrivateTest  \n",
       "2233    ball4real where are you with your miami gayness  PrivateTest  \n",
       "2234  manif you are a 3 point shooter you must love ...  PrivateTest  \n",
       "\n",
       "[2235 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_test_df = preprocess(test_df, True)\n",
    "preprocessed_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we are goint to use the classic Naive Bayes algorithm to classify our data, and try to achieve better results every time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to convert words to vectors and then call the NB algorithm and present the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB_using_CV(train_df, test_df, bigrams, laplace):\n",
    "    # We are going to use the sklearn's count vectorizer\n",
    "    # Check if we want to call useing bigrams\n",
    "    if (bigrams):\n",
    "        cv = CountVectorizer(ngram_range=(1,2))\n",
    "    else:\n",
    "        cv = CountVectorizer()\n",
    "    # Seperate our train data into x and y\n",
    "    x_train = train_df['Processed_Comment']\n",
    "    y_train = train_df.Insult\n",
    "    # find the x for our test set\n",
    "    x_test = test_df['Processed_Comment']\n",
    "    y_test = test_df.Insult\n",
    "    # use the cv in our sets to convert the words\n",
    "    x_train = cv.fit_transform(x_train)\n",
    "    x_test = cv.transform(x_test)\n",
    "    # Call the NB algorithm\n",
    "    if (laplace):\n",
    "        # if we want a laplace smoothing, call multinomial nb, with the default alpha parameter of 1\n",
    "        clf = MultinomialNB()\n",
    "    else:\n",
    "        # else, just call the gaussian nb\n",
    "        clf = GaussianNB()\n",
    "    clf.fit(x_train.toarray(), y_train)\n",
    "    y_pred = clf.predict(x_test.toarray())\n",
    "    # Print the results\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    accuracy = accuracy_score(y_test, y_pred);\n",
    "    print('Accuracy is {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First try: no features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: All the modification of the data will be __temporary__, thus we are going to store the changes of each stage in a new column, named \"processed_comment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Processed_Comment'] = train_df.Comment\n",
    "test_df['Processed_Comment'] = test_df.Comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, without any preprocessing(apart from the one we did at the beggining), let's try and see the results of the NB algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.46      0.50      1158\n",
      "           1       0.51      0.61      0.56      1077\n",
      "\n",
      "    accuracy                           0.53      2235\n",
      "   macro avg       0.54      0.53      0.53      2235\n",
      "weighted avg       0.54      0.53      0.53      2235\n",
      "\n",
      "Accuracy is 0.532\n"
     ]
    }
   ],
   "source": [
    "NB_using_CV(train_df, test_df, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our results are not that great. Let's try to do some preprocessing to the data. We're gonna start by lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second try: Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a lemmatization function that is easy to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lemmatization(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_sentence = []\n",
    "    token_words = word_tokenize(text)\n",
    "    lem_sentence = [lemmatizer.lemmatize(word) for word in token_words]\n",
    "    text = \" \".join(lem_sentence)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the lemmatizing in our data, and check again for better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.45      0.50      1158\n",
      "           1       0.51      0.62      0.56      1077\n",
      "\n",
      "    accuracy                           0.53      2235\n",
      "   macro avg       0.53      0.53      0.53      2235\n",
      "weighted avg       0.53      0.53      0.53      2235\n",
      "\n",
      "Accuracy is 0.529\n"
     ]
    }
   ],
   "source": [
    "# Apply the lemmatization in both our sets\n",
    "vfunc = np.vectorize(Lemmatization);\n",
    "train_df['Processed_Comment'] = vfunc(train_df['Processed_Comment'].values)\n",
    "test_df['Processed_Comment'] = vfunc(test_df['Processed_Comment'].values)\n",
    "#Re-run the NB algorithm\n",
    "NB_using_CV(train_df, test_df, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are just as horrible as before. Lets try another technique: remove all the stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third try: Remove stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_stop_words(text):\n",
    "    removed = remove_stopwords(text)\n",
    "    text = \"\".join(removed)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the method in our data, and check again for better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.44      0.49      1158\n",
      "           1       0.51      0.63      0.56      1077\n",
      "\n",
      "    accuracy                           0.53      2235\n",
      "   macro avg       0.53      0.53      0.53      2235\n",
      "weighted avg       0.53      0.53      0.53      2235\n",
      "\n",
      "Accuracy is 0.529\n"
     ]
    }
   ],
   "source": [
    "# Apply the sotp words remover in both our sets\n",
    "vfunc = np.vectorize(Remove_stop_words);\n",
    "train_df['Processed_Comment'] = vfunc(train_df['Processed_Comment'].values)\n",
    "test_df['Processed_Comment'] = vfunc(test_df['Processed_Comment'].values)\n",
    "#Re-run the NB algorithm\n",
    "NB_using_CV(train_df, test_df, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth try: Use bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check the results if we give our algorithm bigrams, instead of words. This techinque is known for producing better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.49      0.53      1158\n",
      "           1       0.52      0.60      0.56      1077\n",
      "\n",
      "    accuracy                           0.54      2235\n",
      "   macro avg       0.55      0.55      0.54      2235\n",
      "weighted avg       0.55      0.54      0.54      2235\n",
      "\n",
      "Accuracy is 0.544\n"
     ]
    }
   ],
   "source": [
    "# Run the algorithm by passing \"true\" as a parameter, so the cv runs with a ngram_range as an argument\n",
    "NB_using_CV(train_df, test_df, True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last try: Use LaPlace smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will try a technique called LaPlace smoothing, just by passing a different alpha parameter to the NB algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.88      0.75      1158\n",
      "           1       0.79      0.49      0.60      1077\n",
      "\n",
      "    accuracy                           0.69      2235\n",
      "   macro avg       0.72      0.68      0.67      2235\n",
      "weighted avg       0.72      0.69      0.68      2235\n",
      "\n",
      "Accuracy is 0.690\n"
     ]
    }
   ],
   "source": [
    "# as a matter of fact, the correct parameter for alpha is 1\n",
    "alpha = 1\n",
    "# RUn the algorithm again\n",
    "NB_using_CV(train_df, test_df, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, we are going to add more features in order to achieve better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part Of Speech Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, by the way, drop the processed comment, we're not gonna need it anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20120603163526Z</td>\n",
       "      <td>like this if you are a tribe fan</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20120531215447Z</td>\n",
       "      <td>youre idiot</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>20120823164228Z</td>\n",
       "      <td>i am a woman babs and the only war on women i ...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>20120826010752Z</td>\n",
       "      <td>wow  you benefitted so many wins this year fro...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>20120602223825Z</td>\n",
       "      <td>haha green me red you now loser whos winning n...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2230</th>\n",
       "      <td>2231</td>\n",
       "      <td>0</td>\n",
       "      <td>20120528100303Z</td>\n",
       "      <td>fuckin lame come on wtf stop fucking over my b...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2231</th>\n",
       "      <td>2232</td>\n",
       "      <td>1</td>\n",
       "      <td>20120531185813Z</td>\n",
       "      <td>you shut your ignorant pie hole you little ins...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2232</th>\n",
       "      <td>2233</td>\n",
       "      <td>0</td>\n",
       "      <td>20120529130822Z</td>\n",
       "      <td>sweetie pie is looking very much like her cous...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2233</th>\n",
       "      <td>2234</td>\n",
       "      <td>1</td>\n",
       "      <td>20120531045826Z</td>\n",
       "      <td>ball4real where are you with your miami gayness</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2234</th>\n",
       "      <td>2235</td>\n",
       "      <td>0</td>\n",
       "      <td>20120531184524Z</td>\n",
       "      <td>manif you are a 3 point shooter you must love ...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2235 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  Insult             Date  \\\n",
       "0        1       0  20120603163526Z   \n",
       "1        2       1  20120531215447Z   \n",
       "2        3       1  20120823164228Z   \n",
       "3        4       1  20120826010752Z   \n",
       "4        5       1  20120602223825Z   \n",
       "...    ...     ...              ...   \n",
       "2230  2231       0  20120528100303Z   \n",
       "2231  2232       1  20120531185813Z   \n",
       "2232  2233       0  20120529130822Z   \n",
       "2233  2234       1  20120531045826Z   \n",
       "2234  2235       0  20120531184524Z   \n",
       "\n",
       "                                                Comment        Usage  \n",
       "0                      like this if you are a tribe fan  PrivateTest  \n",
       "1                                           youre idiot  PrivateTest  \n",
       "2     i am a woman babs and the only war on women i ...  PrivateTest  \n",
       "3     wow  you benefitted so many wins this year fro...  PrivateTest  \n",
       "4     haha green me red you now loser whos winning n...  PrivateTest  \n",
       "...                                                 ...          ...  \n",
       "2230  fuckin lame come on wtf stop fucking over my b...  PrivateTest  \n",
       "2231  you shut your ignorant pie hole you little ins...  PrivateTest  \n",
       "2232  sweetie pie is looking very much like her cous...  PrivateTest  \n",
       "2233    ball4real where are you with your miami gayness  PrivateTest  \n",
       "2234  manif you are a 3 point shooter you must love ...  PrivateTest  \n",
       "\n",
       "[2235 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.drop('Processed_Comment', 1)\n",
    "test_df.drop('Processed_Comment', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create 4 new columns in our dataframe, each one containing the fraction of a part of speech from the following: noun, verb, adverb, adjective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 4 fumctions to extract the pos percentages\n",
    "def count_nouns(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    count = sum ([1 for char in pos if (char[1] == \"NN\" or char[1] == \"NNS\" or char[1] == \"NNP\" or char[1] == \"NNPS\")])\n",
    "    if (len(pos) != 0):\n",
    "        return round(count/(len(pos)), 3)*100\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def count_verbs(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    count = sum ([1 for char in pos if (char[1] == \"VB\" or char[1] == \"VBD\" or char[1] == \"VBG\" or char[1] == \"VBN\" or char[1] == \"VBP\" or char[1] == \"VBZ\")])\n",
    "    if (len(pos) != 0):\n",
    "        return round(count/(len(pos)), 3)*100\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def count_adverbs(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    count = sum ([1 for char in pos if (char[1] == \"RB\" or char[1] == \"RBR\" or char[1] == \"RBS\")])\n",
    "    if (len(pos) != 0):\n",
    "        return round(count/(len(pos)), 3)*100\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def count_adjectives(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    count = sum ([1 for char in pos if (char[1] == \"JJ\" or char[1] == \"JJR\" or char[1] == \"JJS\")])\n",
    "    if (len(pos) != 0):\n",
    "        return round(count/(len(pos)), 3)*100\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 4 new columns in each set to store the new info that we gathered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['noun%'] = train_df.Comment.apply(lambda row : count_nouns(row))\n",
    "test_df['noun%'] = train_df.Comment.apply(lambda row : count_nouns(row))\n",
    "\n",
    "train_df['verb%'] = train_df.Comment.apply(lambda row : count_verbs(row))\n",
    "test_df['verb%'] = train_df.Comment.apply(lambda row : count_verbs(row))\n",
    "\n",
    "train_df['adverb%'] = train_df.Comment.apply(lambda row : count_adverbs(row))\n",
    "test_df['adverb%'] = train_df.Comment.apply(lambda row : count_adverbs(row))\n",
    "\n",
    "train_df['adjectives%'] = train_df.Comment.apply(lambda row : count_adjectives(row))\n",
    "test_df['adjectives%'] = train_df.Comment.apply(lambda row : count_adjectives(row))\n",
    "\n",
    "train_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the new columns of our dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Usage</th>\n",
       "      <th>Processed_Comment</th>\n",
       "      <th>noun%</th>\n",
       "      <th>verb%</th>\n",
       "      <th>adverb%</th>\n",
       "      <th>adjectives%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20120603163526Z</td>\n",
       "      <td>like this if you are a tribe fan</td>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>like tribe fan</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20120531215447Z</td>\n",
       "      <td>youre idiot</td>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>youre idiot</td>\n",
       "      <td>26.7</td>\n",
       "      <td>26.7</td>\n",
       "      <td>6.7</td>\n",
       "      <td>6.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>20120602223825Z</td>\n",
       "      <td>haha green me red you now loser whos winning n...</td>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>haha green red loser winning moron</td>\n",
       "      <td>71.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>20120603202442Z</td>\n",
       "      <td>nme and god both hatefaggotsnnwhats the differ...</td>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>nme god hatefaggotsnnwhats difference afagand ...</td>\n",
       "      <td>16.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>16.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>20120603163604Z</td>\n",
       "      <td>oh go kiss the ass of a goatand you dummycraps...</td>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>oh kiss goatand dummycraps insult veteranseven...</td>\n",
       "      <td>16.7</td>\n",
       "      <td>16.7</td>\n",
       "      <td>16.7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>2230</td>\n",
       "      <td>1</td>\n",
       "      <td>20120825233538Z</td>\n",
       "      <td>you must have crack in your bong as it aint weed</td>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>crack bong aint weed</td>\n",
       "      <td>28.1</td>\n",
       "      <td>21.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>2231</td>\n",
       "      <td>0</td>\n",
       "      <td>20120528100303Z</td>\n",
       "      <td>fuckin lame come on wtf stop fucking over my b...</td>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fuckin lame come wtf stop fucking bity shit</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>2232</td>\n",
       "      <td>1</td>\n",
       "      <td>20120531185813Z</td>\n",
       "      <td>you shut your ignorant pie hole you little ins...</td>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>shut ignorant pie hole little insignificunt in...</td>\n",
       "      <td>33.3</td>\n",
       "      <td>13.3</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>2233</td>\n",
       "      <td>0</td>\n",
       "      <td>20120529130822Z</td>\n",
       "      <td>sweetie pie is looking very much like her cous...</td>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>sweetie pie looking like cousin maggie ewhat b...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1830</th>\n",
       "      <td>2234</td>\n",
       "      <td>1</td>\n",
       "      <td>20120531045826Z</td>\n",
       "      <td>ball4real where are you with your miami gayness</td>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>ball4real miami gayness</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1831 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  Insult             Date  \\\n",
       "0        1       0  20120603163526Z   \n",
       "1        2       1  20120531215447Z   \n",
       "2        5       1  20120602223825Z   \n",
       "3        6       0  20120603202442Z   \n",
       "4        7       1  20120603163604Z   \n",
       "...    ...     ...              ...   \n",
       "1826  2230       1  20120825233538Z   \n",
       "1827  2231       0  20120528100303Z   \n",
       "1828  2232       1  20120531185813Z   \n",
       "1829  2233       0  20120529130822Z   \n",
       "1830  2234       1  20120531045826Z   \n",
       "\n",
       "                                                Comment        Usage  \\\n",
       "0                      like this if you are a tribe fan  PrivateTest   \n",
       "1                                           youre idiot  PrivateTest   \n",
       "2     haha green me red you now loser whos winning n...  PrivateTest   \n",
       "3     nme and god both hatefaggotsnnwhats the differ...  PrivateTest   \n",
       "4     oh go kiss the ass of a goatand you dummycraps...  PrivateTest   \n",
       "...                                                 ...          ...   \n",
       "1826   you must have crack in your bong as it aint weed  PrivateTest   \n",
       "1827  fuckin lame come on wtf stop fucking over my b...  PrivateTest   \n",
       "1828  you shut your ignorant pie hole you little ins...  PrivateTest   \n",
       "1829  sweetie pie is looking very much like her cous...  PrivateTest   \n",
       "1830    ball4real where are you with your miami gayness  PrivateTest   \n",
       "\n",
       "                                      Processed_Comment  noun%  verb%  \\\n",
       "0                                        like tribe fan   25.0   25.0   \n",
       "1                                           youre idiot   26.7   26.7   \n",
       "2                    haha green red loser winning moron   71.2    4.5   \n",
       "3     nme god hatefaggotsnnwhats difference afagand ...   16.7   14.6   \n",
       "4     oh kiss goatand dummycraps insult veteranseven...   16.7   16.7   \n",
       "...                                                 ...    ...    ...   \n",
       "1826                               crack bong aint weed   28.1   21.1   \n",
       "1827        fuckin lame come wtf stop fucking bity shit    0.0   66.7   \n",
       "1828  shut ignorant pie hole little insignificunt in...   33.3   13.3   \n",
       "1829  sweetie pie looking like cousin maggie ewhat b...   26.0   20.0   \n",
       "1830                            ball4real miami gayness   25.0   25.0   \n",
       "\n",
       "      adverb%  adjectives%  \n",
       "0         0.0          0.0  \n",
       "1         6.7          6.7  \n",
       "2         0.0         19.7  \n",
       "3         2.1         16.7  \n",
       "4        16.7          0.0  \n",
       "...       ...          ...  \n",
       "1826      3.5          3.5  \n",
       "1827      0.0          0.0  \n",
       "1828      6.7          0.0  \n",
       "1829      8.0          8.0  \n",
       "1830      5.0         20.0  \n",
       "\n",
       "[1831 rows x 10 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, we are going to transform our data to a feature vector. In order to do that, we're gonna use TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First for our train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>13378</th>\n",
       "      <th>13379</th>\n",
       "      <th>13380</th>\n",
       "      <th>13381</th>\n",
       "      <th>13382</th>\n",
       "      <th>13383</th>\n",
       "      <th>13384</th>\n",
       "      <th>13385</th>\n",
       "      <th>13386</th>\n",
       "      <th>13387</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 13388 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2      3      4      5      6      7      8      9      ...  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "2    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "4    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "\n",
       "   13378  13379  13380  13381  13382  13383  13384  13385  13386  13387  \n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "2    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[5 rows x 13388 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec = TfidfVectorizer()\n",
    "tfidf_train_vec = tfidf_vec.fit_transform(train_df.Comment)\n",
    "tfidf_train_data = pd.DataFrame(tfidf_train_vec.toarray())\n",
    "tfidf_train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>13378</th>\n",
       "      <th>13379</th>\n",
       "      <th>13380</th>\n",
       "      <th>13381</th>\n",
       "      <th>13382</th>\n",
       "      <th>13383</th>\n",
       "      <th>13384</th>\n",
       "      <th>13385</th>\n",
       "      <th>13386</th>\n",
       "      <th>13387</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 13388 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2      3      4      5      6      7      8      9      ...  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "2    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "4    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "\n",
       "   13378  13379  13380  13381  13382  13383  13384  13385  13386  13387  \n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "2    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[5 rows x 13388 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec_test = tfidf_vec.transform(test_df.Comment)\n",
    "tfidf_test_data = pd.DataFrame(tfidf_vec_test.toarray())\n",
    "tfidf_test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add all the features that we extracted in our final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_data = pd.concat([train_df['noun%'], train_df['verb%'], train_df['adverb%'], train_df['adjectives%'], tfidf_train_data], axis=1)\n",
    "final_test_data = pd.concat([test_df['noun%'], test_df['verb%'], test_df['adverb%'], test_df['adjectives%'], tfidf_test_data], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using SVM and RandomForests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've extracted those features, we are going to check them with 2 different classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first classifier is SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(probability=True, kernel='rbf')\n",
    "y_train = train_df.Insult\n",
    "clf.fit(final_train_data, y_train)\n",
    "y_pred = clf.predict(final_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      1.00      0.69       955\n",
      "           1       0.00      0.00      0.00       876\n",
      "\n",
      "    accuracy                           0.52      1831\n",
      "   macro avg       0.26      0.50      0.34      1831\n",
      "weighted avg       0.27      0.52      0.36      1831\n",
      "\n",
      "Accuracy is 0.522\n"
     ]
    }
   ],
   "source": [
    "y_test = test_df.Insult\n",
    "# Print the results\n",
    "print(classification_report(y_test,y_pred))\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy is {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're gonna try another classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "y_train = train_df.Insult\n",
    "clf.fit(final_train_data, y_train)\n",
    "y_pred = clf.predict(final_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.97      0.74       955\n",
      "           1       0.89      0.28      0.42       876\n",
      "\n",
      "    accuracy                           0.64      1831\n",
      "   macro avg       0.74      0.62      0.58      1831\n",
      "weighted avg       0.73      0.64      0.59      1831\n",
      "\n",
      "Accuracy is 0.637\n"
     ]
    }
   ],
   "source": [
    "y_test = test_df.Insult\n",
    "# Print the results\n",
    "print(classification_report(y_test,y_pred))\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy is {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
